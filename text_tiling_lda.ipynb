{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36d7391-d3e0-497b-aee0-0cae70d3c113",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import MeCab\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5d8250-3b76-4627-a138-7df17eb67140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分かち書きの中で使うオブジェクト生成\n",
    "tagger = MeCab.Tagger(\"-d /usr/local/lib/mecab/dic/mecab-ipadic-neologd\")\n",
    "# ひらがなのみの文字列にマッチする正規表現\n",
    "kana_re = re.compile(\"^[ぁ-ゖ]+$\")\n",
    "\n",
    "\n",
    "def mecab_tokenizer(text):\n",
    "    # テキストを分かち書きする関数を準備する\n",
    "    parsed_lines = tagger.parse(text).split(\"\\n\")[:-2]\n",
    "    surfaces = [l.split('\\t')[0] for l in parsed_lines]\n",
    "    features = [l.split('\\t')[1] for l in parsed_lines]\n",
    "    # 原型を取得\n",
    "    bases = [f.split(',')[6] for f in features]\n",
    "    # 品詞を取得\n",
    "    pos = [f.split(',')[0] for f in features]\n",
    "\n",
    "    # 各単語を原型に変換する\n",
    "    token_list = [b if b != '*' else s for s, b in zip(surfaces, bases)]\n",
    "\n",
    "    # 名詞,動詞,形容詞のみに絞り込み\n",
    "    target_pos = [\"名詞\", \"動詞\", \"形容詞\"]\n",
    "    token_list = [t for t, p in zip(token_list, pos) if p in target_pos]\n",
    "    # アルファベットを小文字に統一\n",
    "    token_list = [t.lower() for t in token_list]\n",
    "    # ひらがなのみの単語を除く\n",
    "    token_list = [t for t in token_list if not kana_re.match(t)]\n",
    "    # 数値を含む単語も除く\n",
    "    token_list = [t for t in token_list if not re.match(\"\\d\", t)]\n",
    "    return \" \".join(token_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d300bf6d-c539-453c-897b-fe07fe429428",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "import MeCab\n",
    "import numpy as np\n",
    "\n",
    "# def calc_similarity(topic1, topic2, window_size=5):\n",
    "def calc_similarity(topic1, topic2, n_components=5):\n",
    "    # テキストデータをBOW形式に変換する\n",
    "    tf_vectorizer = CountVectorizer(\n",
    "        token_pattern='(?u)\\\\b\\\\w+\\\\b',\n",
    "#         max_df=0.90,\n",
    "#         min_df=10,\n",
    "    )\n",
    "    lda_vectorizer = LatentDirichletAllocation(n_components=n_components)\n",
    "    \n",
    "    def train(utterances):\n",
    "        tokenized_utterances = [mecab_tokenizer(utt) for utt in utterances]\n",
    "        tf = tf_vectorizer.fit_transform(tokenized_utterances)\n",
    "        # LDAの学習\n",
    "        lda_vectorizer.fit(tf)\n",
    "    \n",
    "    def predict(utterance):\n",
    "        tokenized_utterance = mecab_tokenizer(utterance)\n",
    "        tf = tf_vectorizer.transform([tokenized_utterance])\n",
    "        return lda_vectorizer.transform(tf)\n",
    "    \n",
    "    train(topic1 + topic2)\n",
    "    \n",
    "    vec1 = predict(' '.join(topic1))\n",
    "    vec2 = predict(' '.join(topic2))\n",
    "    \n",
    "#     vectors = calc_vectors(topic1 + topic2, n_components=n_components)\n",
    "#     n1 = len(topic1)\n",
    "#     vec1 = np.mean(vectors[:n1], axis=0)\n",
    "#     vec2 = np.mean(vectors[n1:], axis=0)\n",
    "\n",
    "#     vec1 = np.mean(vectors[n1-window_size:n1], axis=0)\n",
    "#     vec2 = np.mean(vectors[n1:n1+window_size], axis=0)\n",
    "    return 1 - distance.cosine(vec1, vec2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c74a80-d428-4c3f-89f8-6ee747bca065",
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_similarity(['コンサートとかには行きますか？'], ['コンサートは行かないですけど、映画とかは好きですね'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980a3853-a7ac-4617-8f96-0ce1677d0154",
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_similarity(['コンサートとかには行きますか？'], ['コンサートは行かないですけど、映画とかは好きですね', '映画といえば、閃光のハサウェイは見ましたか？'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d3f189-3d81-48aa-ba7b-3d601080f35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_similarity(['コンサートとかには行きますか？'], ['織田信長の野望'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8859d6d6-2f3f-49f5-b12d-16d3b00d40c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_similarity(['豊臣秀吉の狙いはなんだったのか？'], ['織田信長の野望'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8615de26-4e6b-4b9f-a911-1974ce789ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_generator(text_gen, threshold=0.3, max_utterances=50, min_utterances=10, delay=3, n_components=5):\n",
    "    cur_topic = [next(text_gen)]\n",
    "    \n",
    "    for text in text_gen:\n",
    "        similarity = calc_similarity(cur_topic[:-delay], cur_topic[-delay:] + [text], n_components=n_components)\n",
    "        if (\n",
    "            similarity < threshold\n",
    "            or len(cur_topic) >= max_utterances\n",
    "        ) and len(cur_topic) >= min_utterances + delay:\n",
    "            yield cur_topic[:-delay]\n",
    "            cur_topic = cur_topic[-delay:] + [text]\n",
    "        else:\n",
    "            cur_topic.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc04d013-6dc1-435c-abe0-007d68f8b824",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "import unicodedata\n",
    "import re\n",
    "\n",
    "def transcript_generator(path):\n",
    "    with jsonlines.open(path) as reader:\n",
    "        for data in reader:\n",
    "            yield preprocess_transcript(data['transcript'])\n",
    "\n",
    "def preprocess_transcript(transcript):\n",
    "    transcript = unicodedata.normalize('NFKC', transcript)\n",
    "    transcript = transcript.replace('|', ' ')\n",
    "    transcript = re.sub(r'\\(.+\\)', '', transcript)\n",
    "    return transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40f63ed-e4d8-4313-b774-f55f98b3cc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_gen = topic_generator(\n",
    "    transcript_generator('data/golden_transcripts/1911F2002.jsonl'),\n",
    "    threshold=0.2,\n",
    "    max_utterances=50,\n",
    "    min_utterances=10,\n",
    "    delay=3,\n",
    "    n_components=5\n",
    ")\n",
    "\n",
    "for topic in topic_gen:\n",
    "    print(topic, '\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64c8e5a-7614-484b-82f5-b14e8a192cc5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
